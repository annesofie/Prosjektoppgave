 \chapter{OSM import methods}
%1. Helautomatisk, scriptet import \\
%2. Helmanuell import basert p  tracing av f.eks WMS-data\\
%3. Guidet automatisk import (slik import av n50-data blir gjort i OSM i dag) \\
%4. Import basert p  metodikken LA-buildings-prosjektet har gjort, og som beskrives av McAndrews (microtasking)

%Conflation \href{http://wiki.openstreetmap.org/wiki/Conflation}{Wiki Conflation}
\section{Introduction}
The traditional way to contribute data to the OpenStreetMap project is through active users who use their GPS to track roads and their local knowledge to add information about geographic regions to the OSM database \cite{Zielstra2013}. Users also digitalize aerial photos. Cheaper GPS receivers and more available satellite imagery with better resolution makes it easier for users to contribute \cite{Chilton}. The number of active users in different regions varies a lot, making some regions on the OSM map full of data while others are almost empty. This led to a second approach for getting data in to the OpenStreetMap database, bulk imports \cite{Zielstra2013}.  Bulk import is the process of uploading external data and were meant for initial/preliminary object class uploads, so only if the object were none existing in the area  \cite{Zielstra2013}.  Its a good alternative for countries or regions with few or none active users. Through the years different import methods has been developed. This chapter will evaluate the most common methods. 

\section{Fully automatic import script}
Creating a script that automatically imports big datasets into OpenStreetMap, a bulk import, is not encouraged in the OSM community \cite{Zielstra2013}. This becomes clear when reading the wikipages about import. A bulk import is suppose to be a supplement to user generated data. The user generated data and the users ability to work is always the priority \cite{OSMimport}. A fully automatic import do automated edits to the OpenStreetMap database with little, if any, verification from a human. Automatic edits is changes that has no or very limited human oversight \cite{OSMAutiEdit}. This kind of edits must follow the Automated Edits code of conduct \cite{OSMAutomaticEdits}. The policy was created to prevent damaging acts on the database and ignoring it will result in the import be treated as vandalism. 

An example of a bulk import was the TIGER import. The Topologically Integrated Geographical Encoding and Reference system (TIGER) data was produced by the US Census Bureau and is a public domain data source. The bulk import was completed in early 2008 \cite{Zielstra2013}, populating the nearly empty map of the United States. The TIGER dataset was not perfect and had it's faults, but it was better than no data at all \cite{Willis2008}. The import mainly focused on data containing the general road network for the US \cite{Zielstra2013}. 

Reading through the OpenStreetMap mailing thread called talk, it becomes clear that this import is an automatic edit with very limited oversight. \textit{"Please don't upload Northampton County, [..] I've mapped my entire town there [..]"} \cite{Mielczarek2007}.  Active OSM mappers on the mailing thread tries to save their manuall work . Users not attenting the mailing list had limited, if any, possibilities of their work beeing saved through the import. The mappers importing TIGER data tried not to override existing data. They started on empty states but reading though the "TIGER, which states next?" mailing thread the decisions on which state to be imported next was only based on the people attending the conversations. \textit{"I believe I'm the only one out here in Nebraska [..] Feel free to override my edits"} \cite{Bishop2007}. The import process did not have any requirements on what they should do with existing data. OSM mappers added the TIGER data county by county, state by state. The OSM user Dave Hansen, one of the most active users during this import, created a text file with his upload queue \cite{Hansen2007a} where he added states and counties after requests from users \cite{Hansen2007}\cite{Hansen2007b}. The import team did not have any validation or correction methods or routines. In the aftermath of the import tagging errors have been discovered. For instance, the TIGER data groups residential, local neighbourhood roads, rural roads and city streets into one road class, while OSM uses a more refined data schema with different highway tags for different road classes \cite{Zielstra2013}.
%\textit{"We are processing 6.502308 OSM objects per second"} \cite{ Munro2007}*.

On the TIGER OpenStreetMap wikipage, last updated August 2016, they say it is unlikely that the TIGER data ever will be imported again. The main reason is the growing US mapping community, their mapping is often better than the TIGER data. \textit{"Do not worry about getting your work overwritten by new TIGER data. Go map!"}  \cite{WikiOSMTIGER2007}. A new bulk import with updated TIGER data can overwrite existing, more precise data. The TIGER data are of variable quality, poor road alignment is a huge problem and also wrong highway classification. Many hours of volunteer work could be lost and this is something the community want to avoid. The bulk import in 2007 got the United States on the OSM-Map and saved the mapping community a lot of time finding road names etc. \textit{"TIGER is a skeleton on which we can build some much better maps"} \cite{Willis2007}. On the negative side the project kept the US mappers away, they were told for years that their work was no longer needed after the TIGER upload was complete. But the presence of TIGER data ended up requiring a lot of volunteer help, they needed help fixing errors like the poor road alignments and wrong tagging. 

Bulk imports are overall not recommended today, but have been helpful as well. In the Netherlands bulk imports have met little resistance, mainly because the imports are done by dedicated OpenStreetMap mappers who knew the OSM import guidelines.  Arguments in favor of bulk imports say that a map that already contains some information is easier to work on and can help lower the entry barriers for new contributors. Another argument is that a almost complete map is more attractive for potential users, that again can encourage more use of OSM data in professional terms \cite{Exelvan2010}. But a huge minus to bulk imports are the data aging, since the data being imported often already is a few years old and updating it takes time, often years. The TIGER import was data from 2005, but the import finished in 2008 \cite{Zielstra2013}. Between the time of first import and update the community have fixed bugs, added important metadata, the community would not want to loose that data/information. 

Today there are huge amounts of object types in OpenStreetMap. Bulk imports with limited human interaction do often end up overriding existing data, which is one of the "don't do" points on the import guidelines list on the osm wiki page. OpenStreetMap do not have layers, so data on top of data makes it very difficult to organize and find the data.   
%has common errors like wrong road classifications, adds roads that no longer or never existed, railway and roads 
%moved so they intersect when they do not. 

%\section{Fully manually import based on tracing}
%This method can be very time consuming. The mapping quality depends on the image resolution in the area beeing mapped, it is also hard to add metadata from a image. For instance, its impossible to see the height of a building from a satellite image. 




\section{Guided automatic import}\label{guidedautoimp}
Fully automatic import of huge amounts of data is discouraged in the OSM community, so another approach is guided automatic import. The OSM community encourages people to import only small amounts of data at a time and only after validation and correcting errors \cite{Mehus2014}. This method was used when the OSM-community in Norway got approval from the Norwegian map authority to import N50 data \cite{Kihle2014}. N50 is the official topographic map of Norway. The import process is described on the OSM wikipage. The mapping team would import one municipality at a time. Each municipality dataset were divided up in a .15 deg times .15 deg grid changesets, each changeset contained from five thousand to twenty thousand elements \cite{OSMN502014}. The N50 import was an community import, but only experienced user were encouraged to import the data \cite{Mehus2014}. 

The N50 release was good news for Norway, since regions, especially in northern parts of Norway, had very little data because of few active OSM mappers. This import would then increase the quality of OpenStreetMap in much bigger parts of Norway \href{http://www.digi.no/artikler/kartskatt-til-alle/286539}{kilde}. Its a huge dataset so they had to import it with caution, not everything in the dataset were relevant for OpenStreetMap, this is noted by the OSM user Solhagen href{http://wiki.openstreetmap.org/wiki/User:Solhagen}{kilde}. %Another approach is to convert the sosi files to shp through GDAL and import this shp file into JOSM. This approach is not recommended, converting sosi to shape will result in loosing important data.  SOSI uses a hierarchical meta data structure  

The data was preprocessed by the OSM user tibnor and uploaded to a google drive folder available for everyone. He started on the preprocessing early 2015. Late 2013 the OSM user gnonthgol* created sosi2osm script for convertion between the norwegian dataformat SOSI and OSM file format \href{https://lists.nuug.no/pipermail/kart/2013-October/004314.html}{kilde}. This script is the recommended way of converting the N50 sosi files to the OSM file format. In may 2015 tibnor released a JOSM plugin for the N50 import of rivers/streams to make it faster to check and fix directions. The import process was managed through a wiki page. Here users wrote their name and progression, startdate and enddate of the imported municipality. Elements which needed manual inspection or validation was tagged with "Fix-me" and a description on what to do. They used a python script ("replaceWithOsm.py") to merge N50 data with existing OSM data, adding source=Kartverket N50 tags on the new data. Elements that already exists in OSM, that conflicts with the new data, is marked with FIXME=Merge. Here the user has to search for the conflicting elements and correct the errors manually. Then the data can be uploaded to OSM. 

The N50 import was initially stopped in May 2014 by Paul Norman. The norwegian OSM group started importing the N50 data before consulting with the OSM imports mailing list, which is required. They were also importing the data without the proper approach. This was pointed out by DWG member Paul Norman \cite{Mehus2014}.  DWG is the data working group, created in 2012, and they are authorized by the OSMF to detect and stop imports that are against the import guidelines \cite{OSMDWG}. The Data working group reverted the import because of technical problems and errors in the import \cite{Didriksen2014}. This was a step back for the Norwegian OSM team, they had to start over again. The DWG stopping the import in 2014 was probably for the best, the DWG group has much experience with automated imports. 

The N50 import has been time consuming. It started in 2015 and is still not finished, even though most of the municipalities are imported. The import process was  carried out according to the import guidelines. Without the DWG group the import would probably end up as a automated edit with no proper validation process. This is one example of importing existing data into OpenStreetMap is a time consuming job. There are guidelines to follow, a lot of validations to be done. 


\section{Import based on microtasking (LA-buildings methodolo)}
The N50 import was a good start towards microtasking an import of huge amounts of data. They divided every municipality into .15 degree .15 degree grid changesets and imported one changset at a time. Both the New York bulding import, finished in 2014, and Los Angeles building import, not finished, took this mindset to the next level, creating a Tasking manager interface specific to this import, among other initiatives. The LA-building team created a custom tasking manager to coordinate the LA County building import \cite{OSMTaskingManager}, while the NY-team used the original OSM Tasking Manager. 

The OSM Tasking Manager was created in the aftermath of the Haiti earthquake. This innovation coincided with the growing popularity of microtasking as a solution to manage distributed work \cite{Palen2015}. The Tasking Manager tool was initially created by the newly formed HOT (Humanitarian OpenStreetMap) to help mappers more efficiently coordinate simultanrous work \cite{Palen2015}. The purpose of the tool is to split the mapping into smaller parts, the parts are mapped independently and should be completed in a short timeperiod \cite{HOTa}. The tool is open source and its code is available on Github, making it easier for other projects to create their versions of it. %?The Palen paper from 2015 pointed out improvements that have to be done to the Tasking Manager. One big improvement done is asynchronous communication through comments between mappers working on the same task, this was not present during the NY-import?. 

The Guided automatic import from \ref{guidedautoimp} we saw that dividing datasets into smaller parts makes the import easier to, among others, distribute the workload between experienced users. OSM Tasking manager takes this approach further by among others, offering a graphical user interface around the import. The tasking manager contains important information. Like a description about the import, instructions on how to do the import and important tags etc. It provides an easy way of downloading a dataset, bounded by a grid, into JOSM or id editor. The interface contains a map visualizing which grids are done, which are occupied and how far they have come in the import process. The tasking manager use colors to inform the user if the grid is done, locks it if someone is already working in the grid and also the user easily can see the commit story when they click on a grid. This is important information to the mappers. They can easier work at the same time without worrying about overlapping import. 

The New York building import took 10 months, finishing in June 2014. The project started as a community import, but underestimating the import complexity and time spent training and supporting new mappers they restarted a few months in, loosely forming a group around the project \cite{Barth2014}. The group consisted of volunteer mappers and employees from the Mapbox team. This grouping made coordination easier and also made it easier to ensure proper training \cite{Barth2014a}. More than 20 people spent more than 1500 hours, importing 1 million buildings and over 900 000 addresses \cite{Barth2014}. Common issues during import was written on the Github page. The New York city data was first converted into OSM format, then cut into byte sized blocks which was reviewed and imported manually through the tasking manager, piece by piece. An important validation step was that a different person than the original importer validated the data, reviewing it for errors and cleaning up when needed \cite{Barth2014}. The NY-team developed tasking manager 2 so that tasks can be shaped as arbitrary polygons, rather than automatically squares. 

The LA building import started in 2014. Two OSM enthusiasts started on the project, Jon Schleuss and Omar Ureta. They used code from the NY building import and adopted it to their needs. After a while Mapbox joined in on the import, a important step for the project. Mapbox helped with programming important scripts, converting the data to osm files, creating block groups of the data. First challenge was to decide which datasets to import.  They ended up neglecting address data, which would \textit{delay the project with 1 year or 2} - quote Jon Schleuss, adding only building outlines and building info (assessor data) \cite{Schleuss2016}. They merged and cleaned the datasets, splitting them into blocks and serving the data to the tasking manager. They used mapathons to get the import started. MaptimeLA was the organizer and they also created tutorials for new mappers who joined the team. The first mapathons started with JOSM training to new mappers. Evolving to only arranging import mapathons, or import parties. Through mapathons is was easier for inexperienced mappers to contribute, here they could get the necessary training.  When importing data mappers always have to examine for possible conflicts between existing and new data. If a conflict is found, and the mapper doesn't know how to deal with it, they can flag the .osm file and a more advanced user will look at it. The task will then be finished by someone else. 
%Mapathon example https://www.meetup.com/MaptimeLA/events/233592014/

A big difference between the NY building import and LA building import was that the NY team ended up only allowing some OSM users to import. The NY-import was planned as a community import, but underestimating the import complexity and time spent training and supporting new mappers they restarted a few months in,  they loosely formed a group around the project \cite{Barth2014}. The LA building team allowed everyone to join the import. To keep an track of mappers the LA team created a list where the volunteers had to write their import username \cite{Sambale2016s}. Doing the import job during mapathons is a good idea, it makes it easier to have an overall control over the import process. 

In NY building import, when a error was discovered that required updates to already imported data they had to do an automated edit. Updating existing data manually was very time consuming. Updating OpenStreetMap data programatically, with a script, is according to Alex Barth in Mapbox, crucial for a successful import.  The LA community have pointed out errors that need to be fixed, for instance is Garage incorrectly labelled as houses and condos have been tagged as house not apartments. Reading through the different issues reported on LA building github page are some errors found in already imported data fixed manually and others through scripts. They created a Maproulette challenge on at least one issue, fixing split buildings. The LA team created a script to detect all split buildings and then each detected building were available as a task in Maproulette \cite{Sambale2016a}. Each task was imported into JOSM where they used the plugin \textit{Auto-tools} to merged the split buildings. Maproulette is a gamified approach for fixing OSM errors breaking the error into micro tasks \cite{OpenStreetMap}.  

When using the OSM tasking manager the dataset has to be divided into smaller parts, so that each part has a manageable size for manual uploads. Each part represents one task, and it is important that each task are small enough so that it can be completed in a reasonable time \cite{HOT} **. The NY team created a python script (chunk.py) to divide the data into smaller parts. The script divided the data into the New York City voting districts, there are in total 5258 voting districts, creating 5258 tasks in OSM tasking manager. This was an arbitrary choice, determined by the import team \cite{Barth2014}. The LA building mapper Alan McConchie opened a issue on labuildings github page asking \textit{How to divide up the tasks?} \cite{McConchie2014}. He suggested using census block groups in each county, the grouping gave suitable sized areas for the tasks. Census blocks alone would be too granular tasks, and next level there are tracks, which would result in too big tasks. They used the same script as NY building import (chunk.py). 

The OSM community needs a script which can divide any dataset into smaller blocks, independent on the location of the data. There shouldn't be necessary to spent time on finding existing dataset which can be used to divide the import data into tasks. 

% LA building cleanup https://lists.openstreetmap.org/pipermail/imports/2016-August/004557.html
%Building import Oct 2016 https://lists.openstreetmap.org/pipermail/imports/2016-October/thread.html 
%This is the import OSM mailinglist 

\section{Evaluation}
OpenStreetMap is a large community dependent on active users adding data in their geographic regions. The users have different perspectives on importing data through scripts. In empty regions with few active users a bulk import can be good way of developing the OSM project. But in regions with large amounts of data, there is no agreement on whats best*. Little research has been done on countries where the OSM project relied on data imports to fill the map. The OSM community is undecided on the benefits of bulk imports for the OSM project, especially for areas such as the US where large gorvermental data are freely available \cite{Zielstra2013}.

Validation of the data being imported takes time. Another dimension to bulk imports, with validation especially through tasking manager, is that unrelated issues in the same area gets fixed by the mappers. During the New York building import they fixed 5,000 unrelated map issued along the way \cite{Barth2014}. 

Do the OSM Tasking Manager tool make the import of data into OSM easier for the users? Of course it makes the coordination easier. But looking in the technical perspective maybe not. Or it just tells us that importing data to OSM following all the import guidelines will never be easy. The users that do the import need technical background and deep understanding on how OSM works. 

During the LA import the LA county released new data. This new data was used for rest of the import, but they did not update the already imported regions.  %https://github.com/osmlab/labuildings/issues/111
How old was the NY building data?
%Husk   nevne noe med at flere TIGER fixup tools er utviklet. MapRoulette (maproulette.org) is a micro-tasking/gamification system with several challenges related to TIGER fixup. http://wiki.openstreetmap.org/wiki/TIGER_fixup

