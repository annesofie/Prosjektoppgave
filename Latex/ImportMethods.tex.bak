 \chapter{OSM import methods}\label{ch:importmethods}
To be able to evaluate methods of import and to conclude on if micro-tasking could be the desired method for bulk imports in OpenStreetMap, this study should examine other standard import methods as well. Therefore will this chapter evaluate three common import methods. A fully automatic method, a guided automatic method and the micro-tasking method. This chapter will try to give a good implication if the micro-tasking method can be the desired import method for the OSM community when importing external data.   

\section{Fully automatic import method}
Creating a script that automatically imports big datasets into OpenStreetMap, a bulk import, is not encouraged in the OSM community \cite{Zielstra2013}. This becomes clear when reading the wiki pages about import. %A bulk import is supposed to be a supplement to user generated data. The user-generated data and the user's ability to work is always the priority \cite{OSMimport}. 
A fully automatic import do automated edits to the OpenStreetMap database with little, if any, verification from a human. Automatic edits are changes that have no or very limited human oversight \cite{OSMAutiEdit}. This kind of edits must follow the Automated Edits code of conduct \cite{OSMAutomaticEdits}. The policy was created to prevent damaging acts on the database and ignoring it will result in the import be treated as vandalism. 

An example of a bulk import was the TIGER import. The Topologically Integrated Geographical Encoding and Reference system (TIGER) data was produced by the US Census Bureau and is a public domain data source. The bulk import was completed in early 2008 \cite{Zielstra2013}, populating the nearly empty map of the United States. The TIGER dataset was not perfect and had its faults, but it was better than no data at all \cite{Willis2008}. The import mainly focused on data containing the general road network of the US \cite{Zielstra2013}. 

Reading through the OpenStreetMap mailinglist called talk, it becomes apparent that this import is an automatic edit with very limite oversight. "\textit{Please don't upload Northampton County, [..] I've mapped my entire town there [..]}" \cite{Mielczarek2007}.  Active OSM mappers following the maillinglist are trying to save their manual work. Users not attending the mailing list had limited, if any, possibilities of saving their work through the TIGER import. Mappers importing TIGER data tried not to override existing data. They started on empty states but reading through the "TIGER, which states next?" mailing thread the decisions on which state to import next was solely based on the people attending the conversations. \textit{"I believe I'm the only one out here in Nebraska [..] Feel free to override my edits"} \cite{Bishop2007}. The import process did not have any requirements on what they should do with existing data. OSM mappers added the TIGER data county by county, state by state. The OSM user Dave Hansen, one of the most active users during this import, created a text file with his upload queue and published it on his dev openstreetmap site \cite{Hansen2007a}. He added states and counties after requests from users \cite{Hansen2007}. The import team did not have any validation or correction methods or routines. In the aftermath of the import, tagging errors was discovered. For instance, the TIGER data groups residential, local neighborhood roads, rural roads and city streets into one road class, while OSM uses a more refined data schema with various highway tags for multiple road classes \cite{Zielstra2013}.

On the OSM \textit{TIGER} wiki page, last updated August 2016, they say \textit{it is unlikely that the TIGER data ever will be imported again}. The main reason is the growing US mapping community, and their mapping is often better than the TIGER data. \textit{"Do not worry about getting your work overwritten by new TIGER data. Go map!"}  \cite{WikiOSMTIGER2007}. A new bulk import with updated TIGER data can overwrite existing, more precise data. The TIGER data are of variable quality, poor road alignment is a huge problem and also wrong highway classification. Many hours of volunteer work could just be lost, and this is something the community want to avoid. The TIGER import in 2007 got the United States on the OSM-Map and saved the mapping community a lot of time finding road names etc. \textit{"TIGER is a skeleton on which we can build some much better maps"} \cite{Willis2007}. On the opposite side the project kept the US mappers away, they were told for years that their work was no longer needed after the TIGER upload was complete. But the presence of TIGER data ended up requiring a lot of volunteer help, fixing errors like the poor road alignments and wrong tagging. 

Bulk imports done through a fully automatic method are overall not recommended today. The lack of a validation process and the overwriting of existing data is a huge problem. A fully automatic import will for instance work on a empty map. Then no data is overwritten and there is no need for validation. Arguments for bulk imports say that a map that already contains some information is easier to work on and can help lower the entry barriers for new contributors. Another argument is that an almost complete map is more attractive for potential users, that again can encourage more use of OSM data in professional terms \cite{Exelvan2010}. But a huge minus to bulk imports is the data aging since the data being imported usually is a few years old and updating it takes time, often years. The TIGER import was data from 2005, but the import finished in 2008 \cite{Zielstra2013}. Between the time of first import and update, the community has fixed bugs, added necessary metadata, and the community would not want to lose that information. 

\section{Guided automatic import method}\label{guidedautoimp}
A fully automatic import of huge amounts of data is discouraged in the OSM community, so another method is guided automatic import. The OSM community encourages people to import only small amounts of data at a time and only after validation and correcting errors \cite{Mehus2014}. This method was applied when the OSM-community in Norway got approval from the Norwegian map authority to import N50 data \cite{Kihle2014}. N50 is the official topographic map of Norway, and provide a good data foundation. The import process is described on the OSM \textit{Topography import for Norway} wiki page. The mapping team imported one municipality at a time. Each municipality dataset was divided up in .15 deg time .15 deg grid changesets, and each changeset contained from five thousand to twenty thousand elements \cite{OSMN502014}. The N50 import was a community import, but especially experienced user was encouraged to import the data \cite{Mehus2014}. 

The N50 release was good news for Norway, since regions, especially in northern parts of Norway, had limited amounts of data because of few active OSM mappers. This import would then increase the quality of OpenStreetMap in much bigger parts of Norway \cite{Jorgenrud2013}. It's a huge dataset, so they had to import it with caution, not everything in the dataset was relevant for OpenStreetMap, this is noted by the OSM user Solhagen \cite{Solhagen2015}. %Another approach is to convert the sosi files to shp through GDAL and import this shp file into JOSM. This approach is not recommended, converting sosi to shape will result in loosing important data.  SOSI uses a hierarchical metadata structure  

The OSM user tibnor preprocessed the data and uploaded it to a google drive folder available for everyone. He started on the preprocessing early 2015. Late 2013 the OSM user gnonthgol created sosi2osm script for conversion between the SOSI format and OSM file format, which he informed about in the sosi2osm mail thread \cite{Gnonthgol2013}. This script is the recommended way of converting the N50 SOSI files since it converts directly from SOSI to OSM XML format. Benefits using the SOSI file format in context of the XML format in OSM is mentioned in section \ref{sec:sosiosm}.

The import process was managed through a wiki page. Here users wrote their name and progression, start date and end date of the imported municipality. Elements which needed manual inspection or validation was tagged with "Fix-me" and a description of what to do. They used a python script ("replaceWithOsm.py") to merge N50 data with existing OSM data, adding source=Kartverket N50 tags on the new data. Elements that already existed in OSM, that conflicted with the new data was marked with a FIXME=Merge tag. Here the user had to search for the conflicting elements and correct the errors manually. Only after fixing the conflicting elements the data could be uploaded into OSM. 

The N50 import was initially stopped in May 2014 by Paul Norman. The Norwegian OSM group started importing the N50 data before consulting with the OSM imports mailing list, which is required. They were also importing the data without the proper approach. The wrong import approach was pointed out by DWG member Paul Norman \cite{Mehus2014}.  DWG is the data working group, created in 2012, and they are authorized by the OSMF to detect and stop imports that are against the import guidelines \cite{OSMDWG}. The Data working group reverted the import because of technical problems and errors in the import \cite{Didriksen2014}. The reset was a step back for the Norwegian OSM team, and they had to start over again. The DWG stopping the import in 2014 was probably for the best. They have much experience with bad automated imports. 

The N50 import has been time-consuming. It started in 2015 and is still not finished, even though most of the municipalities are imported. The import process was carried out according to the import guidelines. Without the DWG group, the import would probably end up as an automated edit with no proper validation process. The N50 import is one example of how time-consuming this process can be. There are guidelines to follow, a lot of validations to be done. % importing existing data into OpenStreetMap is a time-consuming job. 

\section{Micro-tasking import method}\label{sec:importmicro}
The N50 import was a good start towards micro-tasking an import of huge amounts of data. They divided every municipality into .15 degree .15 degree grid changesets and imported one changeset at a time. Both the New York building import finished in 2014, and Los Angeles building import, not finished, took this mindset to the next level, creating a Tasking Manager interface particular to this import, among other initiatives. The LA-building team created a custom tasking manager to coordinate the LA County building import \cite{OSMTaskingManager}, while the NY-team used the original OSM Tasking Manager created by the HOT team. 

In the Guided automatic import from \ref{guidedautoimp} we saw that dividing datasets into smaller parts makes the import easier to, among others, distribute the workload between experienced users. The micro-tasking method together with the OSM Tasking Manager takes this approach further by among others, offering a graphical user interface around the import. The tasking manager contains essential information. Like a description of the import, instructions on how to do the import and which tags to use, etc. It provides an easy way of downloading a dataset, bounded by a grid, into JOSM or id editor.

The New York building import took ten months, finishing in June 2014. The project started as a community import but underestimating the import complexity and time spent training and supporting new mappers they restarted a few months in, loosely forming a group around the project \cite{Barth2014}. The group consisted of volunteer mappers and employees from a company named Mapbox. This grouping made coordination easier and also made it simpler to ensure proper training \cite{Barth2014a}. More than 20 people spent more than 1500 hours, importing 1 million buildings and over 900 000 addresses \cite{Barth2014}. Common issues during import were written on the Github page. The New York City datasets was first converted into the OSM file format, then cut into byte sized blocks which were reviewed and imported manually into OSM through the tasking manager, task by task. An important validation step was that a different person than the original importer validated the data, reviewing it for errors and cleaning up when needed \cite{Barth2014}. 

The LA building import started in 2014. Two OSM enthusiasts started on the project, Jon Schleuss, and Omar Ureta. They used code from the NY building import and adapted it to their needs. After a while, the company Mapbox joined in on this import as well, an important step for the project. Mapbox helped with important programming, creating scripts, converting the data to osm files, and dividing the data into reasonable sized tasks \cite{Schleuss2016}. The first challenge was to decide which datasets to import. They ended up neglecting address data, which would \textit{delay the project with 1 year or 2 - Jon Schleuss}, adding just building outlines and building info (assessor data) \cite{Schleuss2016}. They merged and cleaned the datasets, splitting them into blocks and serving the data to the tasking manager. They used mapathons, coordinated mapping events, to get the import started. The first mapathons started with training new mappers and evolving to only arranging import mapathons. Doing the import job during mapathons was a good idea, making it easier to have an overall control over the import process. When importing data mappers always have to examine for possible conflicts between existing and new data. If a conflict is detected, and the mapper doesn't know how to deal with it, they can flag the .osm file, and a more advanced user will look at it. The task will then be finished by someone else. 
%Mapathon example https://www.meetup.com/MaptimeLA/events/233592014/

A big difference between the NY building import and LA building import was that the NY team ended up only allowing some OSM users to import. The NY-import was planned as a community import, but underestimating the import complexity and time spent training and supporting new mappers they restarted a few months in,  they loosely formed a group around the project \cite{Barth2014}. The LA building team allowed everyone to join the import. To keep track of the volunteer mappers the LA team created a list on GitHub where the volunteers had to write their import username \cite{Sambale2016a}. Here there are about 60 registered volunteers today (December 2016). 

In NY building import, when an error was detected that required updates to already imported data they had to do an automated edit. Updating existing data manually was very time-consuming. Updating OpenStreetMap data programmatically, with a script, is according to Alex Barth in Mapbox, crucial for a successful import.  The LA community has pointed out errors that need to be fixed, for instance, is Garage incorrectly labeled as houses and condos have been tagged as a house, not as apartments. Errors encountered in already imported data are either fixed manually or through scripts, both cases are found reading through issues reported on the LA building GitHub page. The LA team created a Maproulette challenge on at least one issue, correcting split buildings. First, they implemented a script to detect all split buildings and then made each discovered building available as a task in Maproulette \cite{Sambale2016a}. This approach was then using the mico-tasking method for solving errors.  

When using the OSM tasking manager the dataset has to be divided into smaller parts. Each part represents one task, and it is important that each task is small enough so that it can be completed in a reasonable time, introduced in chapter \ref{ch:microtask}. The NY team created a python script (chunk.py) to divide the data into smaller parts. The script divided the data into the New York City voting districts, there are in total 5258 voting districts, creating 5258 tasks in OSM Tasking Manager. Dividing the data into NY voting districts was an arbitrary choice, determined by the import team \cite{Barth2014}. The LA building mapper Alan McConchie opened an issue on LA-buildings GitHub page asking "\textit{How to divide up the tasks?}" \cite{McConchie2014}. He suggested using census block groups in each county, this grouping gave suitably sized areas for the tasks. Census blocks alone would be too granular tasks, and next level there are tracks, which would result in too big tasks. They used the same script as NY building import (chunk.py). 
